# ğŸ† LLMThinkBench Leaderboard

<div align="center">

### **[ğŸ”¥ Launch the Leaderboard â†’](https://ctrl-gaurav.github.io/llmthinkbench.github.io/)**

[![Live Demo](https://img.shields.io/badge/ğŸ”¥-Live%20Demo-red?style=for-the-badge)](https://ctrl-gaurav.github.io/llmthinkbench.github.io/)
[![Framework](https://img.shields.io/badge/ğŸ§ -LLMThinkBench-blue?style=for-the-badge)](https://github.com/ctrl-gaurav/LLMThinkBench)
[![Real-time](https://img.shields.io/badge/ğŸ“Š-Real--time%20Updates-green?style=for-the-badge)](#)
[![Interactive](https://img.shields.io/badge/âš¡-Interactive%20Charts-purple?style=for-the-badge)](#)

### **The Ultimate Real-Time Rankings for LLM Reasoning Efficiency**

_Discover which language models think smart, not hard_

</div>

---

## ğŸŒŸ **What Makes This Special?**

<div align="center">

<table>
<tr>
<td width="33%" align="center">

### ğŸ§® **Revolutionary Metrics**

**Overthinking Score**

F1-harmonic mean of accuracy & token efficiency

_Find models that get it right without the fluff_

</td>
<td width="33%" align="center">

### âš¡ **Real-Time Rankings**

**Live Updates**

Dynamic leaderboard with instant model comparisons

_See the latest evaluations as they happen_

</td>
<td width="33%" align="center">

### ğŸ“Š **Interactive Insights**

**Rich Visualizations**

Beautiful charts, filters, and deep-dive analytics

_Explore data like never before_

</td>
</tr>
</table>

</div>

---

## ğŸ¯ **Why This Leaderboard Exists**

### **The Problem with Traditional Benchmarks**

- âŒ Only measure **accuracy** - ignore efficiency
- âŒ Miss **overthinking** - when models waste tokens
- âŒ No real-time comparison across models
- âŒ Static results that quickly become outdated

### **Our Revolutionary Solution**

- âœ… **Overthinking Score** - balances accuracy + efficiency
- âœ… **Live rankings** - see latest model performance
- âœ… **Interactive filters** - find models for your needs
- âœ… **Rich analytics** - understand WHY models perform differently

---

## ğŸ† **Featured Models**

<div align="center">

### **ğŸ”¥ Current Top Performers**

| Rank | Model             | Overthinking Score | Accuracy | Efficiency | Status  |
| ---- | ----------------- | ------------------ | -------- | ---------- | ------- |
| ğŸ¥‡   | **GPT-4.1-mini**  | 0.768              | 90.23%   | 98.14%     | ğŸŸ¢ Live |
| ğŸ¥ˆ   | **GPT-4.1**       | 0.752              | 89.88%   | 97.79%     | ğŸŸ¢ Live |
| ğŸ¥‰   | **GPT-4o**        | 0.737              | 87.56%   | 99.42%     | ğŸŸ¢ Live |
| 4th  | **GPT-4.1-nano**  | 0.713              | 75.35%   | 95.58%     | ğŸŸ¢ Live |
| 5th  | **Llama-3.1-70B** | 0.691              | 75.43%   | 98.12%     | ğŸŸ¢ Live |

_Rankings update automatically as new evaluations complete_

</div>

---

## ğŸ“Š **Leaderboard Features**

### ğŸ›ï¸ **Interactive Controls**

- **ğŸ” Model Search** - Find specific models instantly
- **ğŸ“Š Metric Filters** - Sort by accuracy, efficiency, or score
- **ğŸ·ï¸ Task Categories** - Filter by arithmetic, logic, statistics
- **ğŸ“ˆ Time Range** - View historical performance trends
- **ğŸ¯ Model Comparison** - Side-by-side analysis

### ğŸ“ˆ **Rich Visualizations**

- **ğŸ“Š Scatter Plots** - Accuracy vs Efficiency analysis
- **ğŸ† Ranking Charts** - Dynamic leaderboard tables
- **ğŸ“‰ Performance Trends** - Historical score evolution
- **ğŸ¯ Task Breakdown** - Per-task performance heatmaps
- **âš¡ Efficiency Analysis** - Token usage distributions

### ğŸ”„ **Live Features**

- **ğŸŸ¢ Real-time Updates** - Auto-refresh every 30 seconds
- **ğŸ“± Mobile Responsive** - Perfect on any device
- **ğŸŒ™ Dark/Light Mode** - Toggle for your preference
- **ğŸ“¥ Export Data** - Download results as CSV/JSON
- **ğŸ”— Shareable Links** - Direct links to model comparisons

---

## ğŸ¨ **Visual Highlights**

<div align="center">

### **ğŸŒˆ Beautiful Interface**

**Features:**

- ğŸ¨ **Modern UI** - Clean, professional design
- âœ¨ **Smooth animations** - Delightful interactions
- ğŸŒˆ **Color-coded metrics** - Instant visual understanding
- ğŸ“± **Responsive design** - Works on all devices
- ğŸŒ™ **Dark mode support** - Easy on the eyes

</div>

---

## ğŸ§® **Understanding the Metrics**

### **ğŸ† Overthinking Score Explained**

<div align="center">

```
Overthinking Score = 2 Ã— (Accuracy Ã— Token_Efficiency) / (Accuracy + Token_Efficiency)
```

**Where:**

- `Token_Efficiency = 1 - normalized_tokens`
- `normalized_tokens = (tokens - min_tokens) / (max_tokens - min_tokens)`

</div>

### **ğŸ“Š What Each Score Means**

| Score Range     | Interpretation     | Performance Level                              |
| --------------- | ------------------ | ---------------------------------------------- |
| **0.90 - 1.00** | ğŸŒŸ **Exceptional** | Perfect balance of accuracy & efficiency       |
| **0.80 - 0.89** | ğŸ”¥ **Excellent**   | High performance with good efficiency          |
| **0.70 - 0.79** | âœ… **Good**        | Solid performance, some room for improvement   |
| **0.60 - 0.69** | âš ï¸ **Average**     | Acceptable but inefficient or inaccurate       |
| **< 0.60**      | âŒ **Poor**        | Significant issues with accuracy or efficiency |

### **ğŸ¯ Model Categories**

- **ğŸš€ Efficiency Champions** - High scores with minimal tokens
- **ğŸ¯ Accuracy Leaders** - Near-perfect correctness
- **âš–ï¸ Balanced Performers** - Great all-around models
- **âš¡ Speed Demons** - Fast inference with good results
- **ğŸ§  Reasoning Experts** - Excel at complex mathematical tasks

---

## ğŸ“± **How to Use the Leaderboard**

### **ğŸ” Finding the Perfect Model**

1. **ğŸ¯ Define Your Needs**

   - High accuracy for critical applications?
   - Fast inference for production use?
   - Balanced performance for general use?

2. **ğŸ“Š Use Filters**

   - Sort by Overthinking Score for best overall
   - Filter by task type for specific capabilities
   - Set accuracy/efficiency thresholds

3. **ğŸ“ˆ Compare Models**

   - Select multiple models for side-by-side comparison
   - View detailed breakdown by task category
   - Analyze trade-offs between metrics

4. **ğŸ“¥ Export Results**
   - Download filtered results as CSV
   - Share specific model comparisons
   - Integrate data into your research

---

## ğŸš€ **Getting Started**

### **ğŸŒ Visit the Live Leaderboard**

<div align="center">

### **[ğŸ”¥ Launch Leaderboard â†’](https://ctrl-gaurav.github.io/llmthinkbench.github.io/)**

</div>

### **ğŸ“Š Example Use Cases**

<details>
<summary><b>ğŸ¯ For Researchers</b></summary>

- **Compare model architectures** across reasoning tasks
- **Identify efficiency patterns** in different model families
- **Analyze performance trends** over time
- **Export data** for academic papers and analysis

</details>

<details>
<summary><b>ğŸ¢ For Practitioners</b></summary>

- **Select production models** based on accuracy/cost trade-offs
- **Monitor model performance** across different task types
- **Evaluate new models** against existing benchmarks
- **Make data-driven decisions** for model deployment

</details>

<details>
<summary><b>ğŸ“ˆ For Model Developers</b></summary>

- **Benchmark your models** against state-of-the-art
- **Identify improvement opportunities** in reasoning tasks
- **Track efficiency optimizations** over model versions
- **Submit evaluation results** to join the leaderboard

</details>

---

## ğŸ”— **Related Resources**

<div align="center">

| Resource                       | Description                                        | Link                                                          |
| ------------------------------ | -------------------------------------------------- | ------------------------------------------------------------- |
| ğŸ§  **LLMThinkBench Framework** | The evaluation framework powering this leaderboard | [GitHub](https://github.com/ctrl-gaurav/LLMThinkBench)        |
| ğŸ“¦ **PyPI Package**            | Install and run evaluations yourself               | [PyPI](https://pypi.org/project/llmthinkbench/)               |
| ğŸ“š **Documentation**           | Complete guides and API reference                  | [Docs](https://github.com/ctrl-gaurav/LLMThinkBench#readme)   |
| ğŸ¤ **Contribute**              | Submit your model evaluations                      | [Issues](https://github.com/ctrl-gaurav/LLMThinkBench/issues) |

</div>

---

## ğŸ’¡ **Why Overthinking Matters**

### **ğŸ¯ Real-World Impact**

<div align="center">

<table>
<tr>
<td width="50%">

### **ğŸ’° Cost Efficiency**

- **Token costs** add up quickly in production
- **Faster inference** = better user experience
- **Efficient models** = lower operational costs

</td>
<td width="50%">

### **âš¡ Performance Impact**

- **Response time** matters for users
- **Compute resources** are limited
- **Scalability** requires efficiency

</td>
</tr>
</table>

</div>

### **ğŸ“Š Example: Production Scenario**

```
Scenario: 1M API calls per day

Model A: 99% accuracy, 500 tokens average
Cost: $50/day, 2.5s average response time

Model B: 97% accuracy, 150 tokens average
Cost: $15/day, 0.8s average response time

Overthinking Score reveals Model B is better for production! ğŸ¯
```

---

## ğŸŒŸ **Community & Updates**

### **ğŸ“Š Submit Your Model**

Want to see your model on the leaderboard?

1. **ğŸ”§ Install LLMThinkBench**

   ```bash
   pip install llmthinkbench
   ```

2. **ğŸš€ Run Evaluation**

   ```bash
   llmthinkbench --model_id "your/model" --tasks "sorting comparison sum"
   ```

3. **ğŸ“¤ Submit Results**
   Open an issue with your evaluation results!

### **ğŸ”” Stay Updated**

- **â­ Star** the [LLMThinkBench repository](https://github.com/ctrl-gaurav/LLMThinkBench)
- **ğŸ‘€ Watch** for new model additions
- **ğŸ¦ Follow** for leaderboard updates
- **ğŸ’¬ Join** the discussion in issues

---

## ğŸ“œ **Citation**

If you use this leaderboard in your research, please cite:

```bibtex
@article{srivastava2025llmthinkbench,
      title = {LLMThinkBench: Towards Basic Math Reasoning and Overthinking in Large Language Models},
      author = {Gaurav Srivastava and Aafiya Hussain and Sriram Srinivasan and Xuan Wang},
      year = {2025},
      eprint = {2507.04023},
      archivePrefix = {arXiv},
      primaryClass = {cs.CL},
      url = {https://arxiv.org/abs/2507.04023}
   }
```

---

<div align="center">

### **ğŸš€ Ready to Explore?**

### **[ğŸ”¥ Launch the Leaderboard â†’](https://ctrl-gaurav.github.io/llmthinkbench.github.io/)**

---

**â­ Star the project â€¢ ğŸ¦ Share with friends â€¢ ğŸš€ Join the LLM efficiency revolution**

_Made with â¤ï¸ by the LLMThinkBench team_

</div>
