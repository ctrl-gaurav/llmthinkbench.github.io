# 🏆 LLMThinkBench Leaderboard

<div align="center">

<img src="https://via.placeholder.com/900x200/0f0f23/4f46e5?text=🧠+LLMThinkBench+Leaderboard+🏆+Live+Rankings+⚡" alt="LLMThinkBench Leaderboard" width="900" style="border-radius: 20px; box-shadow: 0 12px 40px rgba(79, 70, 229, 0.4);"/>

[![Live Demo](https://img.shields.io/badge/🔥-Live%20Demo-red?style=for-the-badge)](https://ctrl-gaurav.github.io/llmthinkbench.github.io/)
[![Framework](https://img.shields.io/badge/🧠-LLMThinkBench-blue?style=for-the-badge)](https://github.com/ctrl-gaurav/LLMThinkBench)
[![Real-time](https://img.shields.io/badge/📊-Real--time%20Updates-green?style=for-the-badge)](#)
[![Interactive](https://img.shields.io/badge/⚡-Interactive%20Charts-purple?style=for-the-badge)](#)

### **The Ultimate Real-Time Rankings for LLM Reasoning Efficiency**

*Discover which language models think smart, not hard*

</div>

---

## 🌟 **What Makes This Special?**

<div align="center">

<table>
<tr>
<td width="33%" align="center">

### 🧮 **Revolutionary Metrics**
**Overthinking Score**

F1-harmonic mean of accuracy & token efficiency

*Find models that get it right without the fluff*

</td>
<td width="33%" align="center">

### ⚡ **Real-Time Rankings**
**Live Updates**

Dynamic leaderboard with instant model comparisons

*See the latest evaluations as they happen*

</td>
<td width="33%" align="center">

### 📊 **Interactive Insights**
**Rich Visualizations**

Beautiful charts, filters, and deep-dive analytics

*Explore data like never before*

</td>
</tr>
</table>

</div>

---

## 🎯 **Why This Leaderboard Exists**

### **The Problem with Traditional Benchmarks**
- ❌ Only measure **accuracy** - ignore efficiency
- ❌ Miss **overthinking** - when models waste tokens
- ❌ No real-time comparison across models
- ❌ Static results that quickly become outdated

### **Our Revolutionary Solution**
- ✅ **Overthinking Score** - balances accuracy + efficiency
- ✅ **Live rankings** - see latest model performance
- ✅ **Interactive filters** - find models for your needs
- ✅ **Rich analytics** - understand WHY models perform differently

---

## 🏆 **Featured Models**

<div align="center">

### **🔥 Current Top Performers**

| Rank | Model | Overthinking Score | Accuracy | Efficiency | Status |
|------|-------|-------------------|----------|------------|---------|
| 🥇 | **Qwen3-4B** | 0.925 | 99.1% | 94.2% | 🟢 Live |
| 🥈 | **Claude-3.5-Sonnet** | 0.918 | 98.7% | 93.8% | 🟢 Live |
| 🥉 | **GPT-4-Turbo** | 0.912 | 98.9% | 92.1% | 🟢 Live |
| 4th | **Llama-3.1-70B** | 0.897 | 97.8% | 91.6% | 🟢 Live |
| 5th | **Gemini-Pro** | 0.884 | 97.2% | 90.9% | 🟢 Live |

*Rankings update automatically as new evaluations complete*

</div>

---

## 📊 **Leaderboard Features**

<div align="center">

<img src="https://via.placeholder.com/800x500/1e1b4b/f8fafc?text=📊+Interactive+Dashboard+🔍+Filter+%26+Compare+⚡+Real-time+Updates+📈+Rich+Analytics" alt="Dashboard Preview" width="800" style="border-radius: 15px; box-shadow: 0 8px 32px rgba(30, 27, 75, 0.3);"/>

</div>

### 🎛️ **Interactive Controls**
- **🔍 Model Search** - Find specific models instantly
- **📊 Metric Filters** - Sort by accuracy, efficiency, or score
- **🏷️ Task Categories** - Filter by arithmetic, logic, statistics
- **📈 Time Range** - View historical performance trends
- **🎯 Model Comparison** - Side-by-side analysis

### 📈 **Rich Visualizations**
- **📊 Scatter Plots** - Accuracy vs Efficiency analysis
- **🏆 Ranking Charts** - Dynamic leaderboard tables
- **📉 Performance Trends** - Historical score evolution
- **🎯 Task Breakdown** - Per-task performance heatmaps
- **⚡ Efficiency Analysis** - Token usage distributions

### 🔄 **Live Features**
- **🟢 Real-time Updates** - Auto-refresh every 30 seconds
- **📱 Mobile Responsive** - Perfect on any device
- **🌙 Dark/Light Mode** - Toggle for your preference
- **📥 Export Data** - Download results as CSV/JSON
- **🔗 Shareable Links** - Direct links to model comparisons

---

## 🎨 **Visual Highlights**

<div align="center">

### **🌈 Beautiful Interface**

<img src="https://via.placeholder.com/600x200/3b82f6/ffffff?text=🎨+Modern+Design+✨+Smooth+Animations+🌈+Color-coded+Metrics" alt="UI Preview" width="600" style="border-radius: 10px;"/>

**Features:**
- 🎨 **Modern UI** - Clean, professional design
- ✨ **Smooth animations** - Delightful interactions
- 🌈 **Color-coded metrics** - Instant visual understanding
- 📱 **Responsive design** - Works on all devices
- 🌙 **Dark mode support** - Easy on the eyes

</div>

---

## 🧮 **Understanding the Metrics**

### **🏆 Overthinking Score Explained**

<div align="center">

```
Overthinking Score = 2 × (Accuracy × Token_Efficiency) / (Accuracy + Token_Efficiency)
```

**Where:**
- `Token_Efficiency = 1 - normalized_tokens`
- `normalized_tokens = (tokens - min_tokens) / (max_tokens - min_tokens)`

</div>

### **📊 What Each Score Means**

| Score Range | Interpretation | Performance Level |
|-------------|----------------|------------------|
| **0.90 - 1.00** | 🌟 **Exceptional** | Perfect balance of accuracy & efficiency |
| **0.80 - 0.89** | 🔥 **Excellent** | High performance with good efficiency |
| **0.70 - 0.79** | ✅ **Good** | Solid performance, some room for improvement |
| **0.60 - 0.69** | ⚠️ **Average** | Acceptable but inefficient or inaccurate |
| **< 0.60** | ❌ **Poor** | Significant issues with accuracy or efficiency |

### **🎯 Model Categories**

- **🚀 Efficiency Champions** - High scores with minimal tokens
- **🎯 Accuracy Leaders** - Near-perfect correctness 
- **⚖️ Balanced Performers** - Great all-around models
- **⚡ Speed Demons** - Fast inference with good results
- **🧠 Reasoning Experts** - Excel at complex mathematical tasks

---

## 📱 **How to Use the Leaderboard**

### **🔍 Finding the Perfect Model**

1. **🎯 Define Your Needs**
   - High accuracy for critical applications?
   - Fast inference for production use?
   - Balanced performance for general use?

2. **📊 Use Filters**
   - Sort by Overthinking Score for best overall
   - Filter by task type for specific capabilities
   - Set accuracy/efficiency thresholds

3. **📈 Compare Models**
   - Select multiple models for side-by-side comparison
   - View detailed breakdown by task category
   - Analyze trade-offs between metrics

4. **📥 Export Results**
   - Download filtered results as CSV
   - Share specific model comparisons
   - Integrate data into your research

---

## 🚀 **Getting Started**

### **🌐 Visit the Live Leaderboard**

<div align="center">

### **[🔥 Launch Leaderboard →](https://ctrl-gaurav.github.io/llmthinkbench.github.io/)**

</div>

### **📊 Example Use Cases**

<details>
<summary><b>🎯 For Researchers</b></summary>

- **Compare model architectures** across reasoning tasks
- **Identify efficiency patterns** in different model families
- **Analyze performance trends** over time
- **Export data** for academic papers and analysis

</details>

<details>
<summary><b>🏢 For Practitioners</b></summary>

- **Select production models** based on accuracy/cost trade-offs
- **Monitor model performance** across different task types
- **Evaluate new models** against existing benchmarks
- **Make data-driven decisions** for model deployment

</details>

<details>
<summary><b>📈 For Model Developers</b></summary>

- **Benchmark your models** against state-of-the-art
- **Identify improvement opportunities** in reasoning tasks
- **Track efficiency optimizations** over model versions
- **Submit evaluation results** to join the leaderboard

</details>

---

## 🔗 **Related Resources**

<div align="center">

| Resource | Description | Link |
|----------|-------------|------|
| 🧠 **LLMThinkBench Framework** | The evaluation framework powering this leaderboard | [GitHub](https://github.com/ctrl-gaurav/LLMThinkBench) |
| 📦 **PyPI Package** | Install and run evaluations yourself | [PyPI](https://pypi.org/project/llmthinkbench/) |
| 📚 **Documentation** | Complete guides and API reference | [Docs](https://github.com/ctrl-gaurav/LLMThinkBench#readme) |
| 🤝 **Contribute** | Submit your model evaluations | [Issues](https://github.com/ctrl-gaurav/LLMThinkBench/issues) |

</div>

---

## 💡 **Why Overthinking Matters**

### **🎯 Real-World Impact**

<div align="center">

<table>
<tr>
<td width="50%">

### **💰 Cost Efficiency**
- **Token costs** add up quickly in production
- **Faster inference** = better user experience
- **Efficient models** = lower operational costs

</td>
<td width="50%">

### **⚡ Performance Impact**
- **Response time** matters for users
- **Compute resources** are limited
- **Scalability** requires efficiency

</td>
</tr>
</table>

</div>

### **📊 Example: Production Scenario**

```
Scenario: 1M API calls per day

Model A: 99% accuracy, 500 tokens average
Cost: $50/day, 2.5s average response time

Model B: 97% accuracy, 150 tokens average  
Cost: $15/day, 0.8s average response time

Overthinking Score reveals Model B is better for production! 🎯
```

---

## 🌟 **Community & Updates**

### **📊 Submit Your Model**

Want to see your model on the leaderboard? 

1. **🔧 Install LLMThinkBench**
   ```bash
   pip install llmthinkbench
   ```

2. **🚀 Run Evaluation**
   ```bash
   llmthinkbench --model_id "your/model" --tasks "sorting comparison sum"
   ```

3. **📤 Submit Results**
   Open an issue with your evaluation results!

### **🔔 Stay Updated**

- **⭐ Star** the [LLMThinkBench repository](https://github.com/ctrl-gaurav/LLMThinkBench)
- **👀 Watch** for new model additions
- **🐦 Follow** for leaderboard updates
- **💬 Join** the discussion in issues

---

## 📜 **Citation**

If you use this leaderboard in your research, please cite:

```bibtex
@software{llmthinkbench_leaderboard2025,
  author = {Gaurav Srivastava and Aafiya Hussain and Sriram Srinivasan and Xuan Wang},
  title = {LLMThinkBench Leaderboard: Real-time Rankings for LLM Reasoning Efficiency},
  year = {2025},
  url = {https://ctrl-gaurav.github.io/llmthinkbench.github.io/},
  note = {Interactive leaderboard for LLM overthinking evaluation}
}
```

---

<div align="center">

### **🚀 Ready to Explore?**

### **[🔥 Launch the Leaderboard →](https://ctrl-gaurav.github.io/llmthinkbench.github.io/)**

---

**⭐ Star the project • 🐦 Share with friends • 🚀 Join the LLM efficiency revolution**

*Made with ❤️ by the LLMThinkBench team*

</div>
