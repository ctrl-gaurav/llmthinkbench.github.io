# 🏆 LLMThinkBench Leaderboard

<div align="center">

### **[🔥 Launch the Leaderboard →](https://ctrl-gaurav.github.io/llmthinkbench.github.io/)**

[![Live Demo](https://img.shields.io/badge/🔥-Live%20Demo-red?style=for-the-badge)](https://ctrl-gaurav.github.io/llmthinkbench.github.io/)
[![Framework](https://img.shields.io/badge/🧠-LLMThinkBench-blue?style=for-the-badge)](https://github.com/ctrl-gaurav/LLMThinkBench)
[![Real-time](https://img.shields.io/badge/📊-Real--time%20Updates-green?style=for-the-badge)](#)
[![Interactive](https://img.shields.io/badge/⚡-Interactive%20Charts-purple?style=for-the-badge)](#)

### **The Ultimate Real-Time Rankings for LLM Reasoning Efficiency**

_Discover which language models think smart, not hard_

</div>

---

## 🌟 **What Makes This Special?**

<div align="center">

<table>
<tr>
<td width="33%" align="center">

### 🧮 **Revolutionary Metrics**

**Overthinking Score**

F1-harmonic mean of accuracy & token efficiency

_Find models that get it right without the fluff_

</td>
<td width="33%" align="center">

### ⚡ **Real-Time Rankings**

**Live Updates**

Dynamic leaderboard with instant model comparisons

_See the latest evaluations as they happen_

</td>
<td width="33%" align="center">

### 📊 **Interactive Insights**

**Rich Visualizations**

Beautiful charts, filters, and deep-dive analytics

_Explore data like never before_

</td>
</tr>
</table>

</div>

---

## 🎯 **Why This Leaderboard Exists**

### **The Problem with Traditional Benchmarks**

- ❌ Only measure **accuracy** - ignore efficiency
- ❌ Miss **overthinking** - when models waste tokens
- ❌ No real-time comparison across models
- ❌ Static results that quickly become outdated

### **Our Revolutionary Solution**

- ✅ **Overthinking Score** - balances accuracy + efficiency
- ✅ **Live rankings** - see latest model performance
- ✅ **Interactive filters** - find models for your needs
- ✅ **Rich analytics** - understand WHY models perform differently

---

## 🏆 **Featured Models**

<div align="center">

### **🔥 Current Top Performers**

| Rank | Model             | Overthinking Score | Accuracy | Efficiency | Status  |
| ---- | ----------------- | ------------------ | -------- | ---------- | ------- |
| 🥇   | **GPT-4.1-mini**  | 0.768              | 90.23%   | 98.14%     | 🟢 Live |
| 🥈   | **GPT-4.1**       | 0.752              | 89.88%   | 97.79%     | 🟢 Live |
| 🥉   | **GPT-4o**        | 0.737              | 87.56%   | 99.42%     | 🟢 Live |
| 4th  | **GPT-4.1-nano**  | 0.713              | 75.35%   | 95.58%     | 🟢 Live |
| 5th  | **Llama-3.1-70B** | 0.691              | 75.43%   | 98.12%     | 🟢 Live |

_Rankings update automatically as new evaluations complete_

</div>

---

## 📊 **Leaderboard Features**

### 🎛️ **Interactive Controls**

- **🔍 Model Search** - Find specific models instantly
- **📊 Metric Filters** - Sort by accuracy, efficiency, or score
- **🏷️ Task Categories** - Filter by arithmetic, logic, statistics
- **📈 Time Range** - View historical performance trends
- **🎯 Model Comparison** - Side-by-side analysis

### 📈 **Rich Visualizations**

- **📊 Scatter Plots** - Accuracy vs Efficiency analysis
- **🏆 Ranking Charts** - Dynamic leaderboard tables
- **📉 Performance Trends** - Historical score evolution
- **🎯 Task Breakdown** - Per-task performance heatmaps
- **⚡ Efficiency Analysis** - Token usage distributions

### 🔄 **Live Features**

- **🟢 Real-time Updates** - Auto-refresh every 30 seconds
- **📱 Mobile Responsive** - Perfect on any device
- **🌙 Dark/Light Mode** - Toggle for your preference
- **📥 Export Data** - Download results as CSV/JSON
- **🔗 Shareable Links** - Direct links to model comparisons

---

## 🎨 **Visual Highlights**

<div align="center">

### **🌈 Beautiful Interface**

**Features:**

- 🎨 **Modern UI** - Clean, professional design
- ✨ **Smooth animations** - Delightful interactions
- 🌈 **Color-coded metrics** - Instant visual understanding
- 📱 **Responsive design** - Works on all devices
- 🌙 **Dark mode support** - Easy on the eyes

</div>

---

## 🧮 **Understanding the Metrics**

### **🏆 Overthinking Score Explained**

<div align="center">

```
Overthinking Score = 2 × (Accuracy × Token_Efficiency) / (Accuracy + Token_Efficiency)
```

**Where:**

- `Token_Efficiency = 1 - normalized_tokens`
- `normalized_tokens = (tokens - min_tokens) / (max_tokens - min_tokens)`

</div>

### **📊 What Each Score Means**

| Score Range     | Interpretation     | Performance Level                              |
| --------------- | ------------------ | ---------------------------------------------- |
| **0.90 - 1.00** | 🌟 **Exceptional** | Perfect balance of accuracy & efficiency       |
| **0.80 - 0.89** | 🔥 **Excellent**   | High performance with good efficiency          |
| **0.70 - 0.79** | ✅ **Good**        | Solid performance, some room for improvement   |
| **0.60 - 0.69** | ⚠️ **Average**     | Acceptable but inefficient or inaccurate       |
| **< 0.60**      | ❌ **Poor**        | Significant issues with accuracy or efficiency |

### **🎯 Model Categories**

- **🚀 Efficiency Champions** - High scores with minimal tokens
- **🎯 Accuracy Leaders** - Near-perfect correctness
- **⚖️ Balanced Performers** - Great all-around models
- **⚡ Speed Demons** - Fast inference with good results
- **🧠 Reasoning Experts** - Excel at complex mathematical tasks

---

## 📱 **How to Use the Leaderboard**

### **🔍 Finding the Perfect Model**

1. **🎯 Define Your Needs**

   - High accuracy for critical applications?
   - Fast inference for production use?
   - Balanced performance for general use?

2. **📊 Use Filters**

   - Sort by Overthinking Score for best overall
   - Filter by task type for specific capabilities
   - Set accuracy/efficiency thresholds

3. **📈 Compare Models**

   - Select multiple models for side-by-side comparison
   - View detailed breakdown by task category
   - Analyze trade-offs between metrics

4. **📥 Export Results**
   - Download filtered results as CSV
   - Share specific model comparisons
   - Integrate data into your research

---

## 🚀 **Getting Started**

### **🌐 Visit the Live Leaderboard**

<div align="center">

### **[🔥 Launch Leaderboard →](https://ctrl-gaurav.github.io/llmthinkbench.github.io/)**

</div>

### **📊 Example Use Cases**

<details>
<summary><b>🎯 For Researchers</b></summary>

- **Compare model architectures** across reasoning tasks
- **Identify efficiency patterns** in different model families
- **Analyze performance trends** over time
- **Export data** for academic papers and analysis

</details>

<details>
<summary><b>🏢 For Practitioners</b></summary>

- **Select production models** based on accuracy/cost trade-offs
- **Monitor model performance** across different task types
- **Evaluate new models** against existing benchmarks
- **Make data-driven decisions** for model deployment

</details>

<details>
<summary><b>📈 For Model Developers</b></summary>

- **Benchmark your models** against state-of-the-art
- **Identify improvement opportunities** in reasoning tasks
- **Track efficiency optimizations** over model versions
- **Submit evaluation results** to join the leaderboard

</details>

---

## 🔗 **Related Resources**

<div align="center">

| Resource                       | Description                                        | Link                                                          |
| ------------------------------ | -------------------------------------------------- | ------------------------------------------------------------- |
| 🧠 **LLMThinkBench Framework** | The evaluation framework powering this leaderboard | [GitHub](https://github.com/ctrl-gaurav/LLMThinkBench)        |
| 📦 **PyPI Package**            | Install and run evaluations yourself               | [PyPI](https://pypi.org/project/llmthinkbench/)               |
| 📚 **Documentation**           | Complete guides and API reference                  | [Docs](https://github.com/ctrl-gaurav/LLMThinkBench#readme)   |
| 🤝 **Contribute**              | Submit your model evaluations                      | [Issues](https://github.com/ctrl-gaurav/LLMThinkBench/issues) |

</div>

---

## 💡 **Why Overthinking Matters**

### **🎯 Real-World Impact**

<div align="center">

<table>
<tr>
<td width="50%">

### **💰 Cost Efficiency**

- **Token costs** add up quickly in production
- **Faster inference** = better user experience
- **Efficient models** = lower operational costs

</td>
<td width="50%">

### **⚡ Performance Impact**

- **Response time** matters for users
- **Compute resources** are limited
- **Scalability** requires efficiency

</td>
</tr>
</table>

</div>

### **📊 Example: Production Scenario**

```
Scenario: 1M API calls per day

Model A: 99% accuracy, 500 tokens average
Cost: $50/day, 2.5s average response time

Model B: 97% accuracy, 150 tokens average
Cost: $15/day, 0.8s average response time

Overthinking Score reveals Model B is better for production! 🎯
```

---

## 🌟 **Community & Updates**

### **📊 Submit Your Model**

Want to see your model on the leaderboard?

1. **🔧 Install LLMThinkBench**

   ```bash
   pip install llmthinkbench
   ```

2. **🚀 Run Evaluation**

   ```bash
   llmthinkbench --model_id "your/model" --tasks "sorting comparison sum"
   ```

3. **📤 Submit Results**
   Open an issue with your evaluation results!

### **🔔 Stay Updated**

- **⭐ Star** the [LLMThinkBench repository](https://github.com/ctrl-gaurav/LLMThinkBench)
- **👀 Watch** for new model additions
- **🐦 Follow** for leaderboard updates
- **💬 Join** the discussion in issues

---

## 📜 **Citation**

If you use this leaderboard in your research, please cite:

```bibtex
@article{srivastava2025llmthinkbench,
      title = {LLMThinkBench: Towards Basic Math Reasoning and Overthinking in Large Language Models},
      author = {Gaurav Srivastava and Aafiya Hussain and Sriram Srinivasan and Xuan Wang},
      year = {2025},
      eprint = {2507.04023},
      archivePrefix = {arXiv},
      primaryClass = {cs.CL},
      url = {https://arxiv.org/abs/2507.04023}
   }
```

---

<div align="center">

### **🚀 Ready to Explore?**

### **[🔥 Launch the Leaderboard →](https://ctrl-gaurav.github.io/llmthinkbench.github.io/)**

---

**⭐ Star the project • 🐦 Share with friends • 🚀 Join the LLM efficiency revolution**

_Made with ❤️ by the LLMThinkBench team_

</div>
